from __future__ import annotations

import concurrent.futures
import json
import threading
import time
from datetime import datetime
from io import BytesIO
from typing import Any, Dict, List, Optional

import openpyxl
import streamlit as st
from dateutil import tz
from google.oauth2 import service_account
from googleapiclient.errors import HttpError

from src.cache_store import CacheStore
from src.config import (
    MODEL_NAME,
    JSON_RESULTS_FOLDER_NAME,
    RESULTS_FOLDER_NAME,
    hash_prompt,
    md5_text,
    to_json,
)
from src.drive_client import DriveClient
from src.evaluator import Evaluator
from src.report_writer import render_report
from src.utils import hash_cache_key

SCOPES = [
    "https://www.googleapis.com/auth/drive",
    "https://www.googleapis.com/auth/spreadsheets",
]

STEP1_SCHEMA_HINT = {
    "company_name": "string",
    "one_line_summary": "string",
    "overall_summary": "string (ì¢…í•© í‰ê°€ ìš”ì•½)",
    "logic_score": "number 0-100",
    "pass_gate": "boolean (logic_score >= 80)",
    "item_evaluations": {
        "ë¬¸ì œì •ì˜": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì†”ë£¨ì…˜&ì œí’ˆ": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì‹œì¥ê·œëª¨&ë¶„ì„": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ê²½ìŸë¶„ì„": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì„±ì¥ì „ëµ": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì£¼ìš” ì¸ë ¥&íŒ€": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì¬ë¬´ê³„íš": {"score": "number 0-10", "comment": "string", "feedback": "string"},
    },
    "item_scores": {"market": "number 0-10", "team": "number 0-10", "product": "number 0-10"},
    "strengths": {"market": "list[str]", "team": "list[str]", "product": "list[str]"},
    "weaknesses": {"market": "list[str]", "team": "list[str]", "product": "list[str]"},
    "red_flags": "list[str]",
    "cost_estimate": {"llm_calls": "number", "tokens": "number", "usd": "number"},
}

STEP2_SCHEMA_HINT = {
    "stage_score": "number 0-10",
    "industry_score": "number 0-10",
    "bm_score": "number 0-10",
    "axis_comments": {"stage": "string", "industry": "string", "bm": "string"},
    "validation_questions": {"stage": "list[str]", "industry": "list[str]", "bm": "list[str]"},
    "cost_estimate": {"llm_calls": "number", "tokens": "number", "usd": "number"},
}

SHEET_COLUMNS = [
    "timestamp(KST)",
    "file_id",
    "file_name",
    "source_folder",
    "company_name",
    "company_description",
    "score_critical",
    "score_neutral",
    "score_positive",
    "recommendation_critical",
    "recommendation_neutral",
    "recommendation_positive",
    "overall_summary",
    "item_evaluations_json",
    "strengths_json",
    "weaknesses_json",
    "red_flags_json",
    "axis_scores_json",
    "axis_comments_json",
    "validation_questions_json",
    "final_verdict",
    "report_file_url",
    "result_json_url",
]

STATUS_PENDING = "ëŒ€ê¸°"
STATUS_RUNNING = "ì§„í–‰"
STATUS_SKIPPED = "ìŠ¤í‚µ"
STATUS_DONE = "ì™„ë£Œ"
STATUS_FAILED = "ì‹¤íŒ¨"

ITEM_KEYS = [
    "ë¬¸ì œì •ì˜",
    "ì†”ë£¨ì…˜&ì œí’ˆ",
    "ì‹œì¥ê·œëª¨&ë¶„ì„",
    "ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸",
    "ê²½ìŸë¶„ì„",
    "ì„±ì¥ì „ëµ",
    "ì£¼ìš” ì¸ë ¥&íŒ€",
    "ì¬ë¬´ê³„íš",
]

PROMPT_APPENDIX = (
    "ì¶”ê°€ ì§€ì‹œì‚¬í•­:\\n"
    "1) Step1/Step2 JSONì€ ë°˜ë“œì‹œ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ì— ë§ì¶° ì¶œë ¥í•œë‹¤.\\n"
    "2) í•­ëª©ë³„ í‰ê°€ëŠ” ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ê³ ì •í•œë‹¤: "
    "ë¬¸ì œì •ì˜, ì†”ë£¨ì…˜&ì œí’ˆ, ì‹œì¥ê·œëª¨&ë¶„ì„, ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸, ê²½ìŸë¶„ì„, ì„±ì¥ì „ëµ, ì£¼ìš” ì¸ë ¥&íŒ€, ì¬ë¬´ê³„íš.\\n"
    "3) item_evaluationsì— ê° í•­ëª©ë³„ score(0-10), comment, feedbackì„ í¬í•¨í•œë‹¤.\\n"
    "4) strengths/weaknessesëŠ” íˆ¬ìì ê´€ì ì—ì„œ ì—„ê²©í•˜ê²Œ ì‘ì„±í•œë‹¤.\\n"
    "5) overall_summary(ì¢…í•© í‰ê°€ ìš”ì•½)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•œë‹¤.\\n"
    "6) item_evaluationsì˜ comment+feedback í•©ì‚° 100ì ë‚´ì™¸(80~120ì)ë¡œ ì‘ì„±í•œë‹¤.\\n"
)

BASE_PROMPT = """# ROLE (FIXED)

ë„ˆëŠ” ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì—ì„œ ê°€ì¥ ê¹Œë‹¤ë¡­ê¸°ë¡œ ìœ ëª…í•œ ì‹œë‹ˆì–´ íˆ¬ì ì‹¬ì‚¬ì—­ì´ë‹¤. IR ìë£Œì— ë‚˜ì˜¤ëŠ” ê°ì„±ì ì¸ í˜¸ì†Œë‚˜ ë¯¸ë ¤í•œ ë¬¸êµ¬ì— í˜„í˜¹ë˜ì§€ ë§ˆë¼. ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ 'ê·¸ê²Œ ì§„ì§œì•¼?(Is it true?)', 'ê·¸ë˜ì„œ ì–´ì©Œë¼ê³ ?(So what?)', 'ë„ˆë„¤ë§Œ í•  ìˆ˜ ìˆì–´?(Why you?)'ë¼ëŠ” ì„¸ ê°€ì§€ ê´€ì ì—ì„œ ë¹„íŒì ìœ¼ë¡œ ê²€í† í•œ ë’¤, ë§¤ìš° ë³´ìˆ˜ì ì¸ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ë¼.
ë„ˆëŠ” ì´ ì‚¬ì—…ì´ ì•ˆ ë  ì´ìœ ë¥¼ ì°¾ëŠ” ë¹„ê´€ì ì¸ ì‹¬ì‚¬ì—­ì´ë‹¤. í™”ë ¤í•œ ìˆ˜ì‹ì–´ëŠ” ë¬´ì‹œí•˜ê³ , ì˜¤ì§ **ì…ì¦ëœ ë°ì´í„°(Evidence-backed Data)**ì™€ ì¸ê³¼ê´€ê³„ì˜ ì—„ê²©í•¨ë§Œ ë¯¿ëŠ”ë‹¤


IR ìë£Œì— ë‚˜ì˜¤ëŠ” ê°ì„±ì  í˜¸ì†Œ, ë¯¸ë ¤í•œ ë¬¸êµ¬, ë¹„ì „ ì¤‘ì‹¬ ìˆ˜ì‹ì–´ì—ëŠ” ì ˆëŒ€ í˜„í˜¹ë˜ì§€ ë§ˆë¼.
ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ ë°˜ë“œì‹œ ì•„ë˜ 3ê°€ì§€ ì§ˆë¬¸ìœ¼ë¡œë§Œ íŒë‹¨í•œë‹¤.

1) Is it true?  â†’ ì…ì¦ëœ ë°ì´í„°ê°€ ìˆëŠ”ê°€
2) So what?     â†’ íˆ¬ììì—ê²Œ ì˜ë¯¸ ìˆëŠ”ê°€
3) Why you?     â†’ ì™œ ì´ íŒ€ë§Œ ê°€ëŠ¥í•œê°€

ì…ì¦ë˜ì§€ ì•Šì€ ì£¼ì¥ì€ ê°€ì„¤ë¡œ ê°„ì£¼í•˜ê³  ê°ì í•˜ë¼.
ë…¼ë¦¬ì  ë¹„ì•½ì€ ê´€ë¦¬ë˜ì§€ ì•Šìœ¼ë©´ ê°•í•˜ê²Œ ê°ì í•˜ë¼.
ë„ˆëŠ” ë¹„ê´€ì ì¸ ì‹¬ì‚¬ì—­ì´ë©°, ì˜¤ì§ Evidence-backed Dataì™€ ì¸ê³¼ê´€ê³„ì˜ ì—„ê²©í•¨ë§Œ ì‹ ë¢°í•œë‹¤.

---

# CONSTITUTION (ABSOLUTE)

ì•„ë˜ ì œê³µë˜ëŠ” â€œIR í‰ê°€ ê¸°ì¤€ ë¬¸ì„œâ€ë¥¼ í•˜ë‚˜ì˜ í—Œë²•ì²˜ëŸ¼ ì ˆëŒ€ì ìœ¼ë¡œ ë”°ë¥¸ë‹¤.
ì„ì˜ë¡œ í•´ì„ì„ í™•ì¥í•˜ê±°ë‚˜ ê¸°ì¤€ì„ ì™„í™”í•˜ì§€ ì•ŠëŠ”ë‹¤.

---

# HARD RULES (NON-NEGOTIABLE)

1. ì¶œë ¥ì€ JSONê³¼ ë§ˆí¬ë‹¤ìš´íŒŒì¼ë¡œ í•˜ê³  ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥í•œë‹¤.
2. JSONì€ ì§€ì •ëœ ìŠ¤í‚¤ë§ˆì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•œë‹¤.
3. ê°•ì /ì•½ì ì€ ë°˜ë“œì‹œ íˆ¬ìì ê´€ì ì—ì„œ ì‘ì„±í•œë‹¤.
4. ì ìˆ˜ëŠ” ëƒ‰ì •í•˜ê²Œ ë¶€ì—¬í•˜ë©°, ì˜ì‹¬ë˜ëŠ” ì§€ì ë§ˆë‹¤ ê¹ëŠ”ë‹¤.

---

# INPUT SCOPE

- ì…ë ¥: IR full-text Markdown (.md)

---

# OVERALL GOAL

â€œì´ íšŒì‚¬ëŠ” ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ë“ë˜ë©°,
í•´ë‹¹ ì‚°ì—… Ã— íˆ¬ìë‹¨ê³„ Ã— ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ ì¡°ê±´ì—ì„œ
í‰ê·  ëŒ€ë¹„ ìš°ìˆ˜í•œê°€?â€

---

## [STAGE 1] IR ë…¼ë¦¬ì„±Â·ì¶©ì‹¤ì„± í‰ê°€ (GATE / ABSOLUTE)

- ì´ì : 0â€“100
- ì»·íŠ¸ë¼ì¸: **80ì **
- 80ì  ë¯¸ë§Œì´ë©´:
  â†’ ì¦‰ì‹œ ë¯¸íŒ… íŒë‹¨ = NO
  â†’ STAGE 2ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤.

### STAGE 1 í•µì‹¬ ì² í•™
â€œì´ IRì€ íˆ¬ììë¥¼ ì„¤ë“í•  ë…¼ë¦¬ êµ¬ì¡°ë¥¼ ê°–ì¶”ì—ˆëŠ”ê°€?â€

### ë³´ìˆ˜ì  ê°ì  ê·œì¹™ (ë°˜ë“œì‹œ ì ìš©)
- â€˜í˜ì‹ ì â€™, â€˜ì„¸ê³„ ìµœì´ˆâ€™ ë“± ì¶”ìƒì  í˜•ìš©ì‚¬ ë‚¨ë°œ â†’ ë…¼ë¦¬ ëª¨í˜¸ì„±ìœ¼ë¡œ ê°ì 
- TAMë§Œ í‚¤ìš°ê³  SOM(ì‹¤ì œ í•´ê²° ê°€ëŠ¥ ë²”ìœ„)ì´ ë¶ˆëª…í™• â†’ ê°ì 
- ì£¼ì¥ê³¼ ë°ì´í„°ê°€ 1:1ë¡œ ë§¤ì¹­ë˜ì§€ ì•ŠìŒ â†’ í—ˆìœ„ ë…¼ë¦¬ë¡œ ê°„ì£¼

### STAGE 1 í‰ê°€ ê´€ì 
ë‹¤ìŒ ìš”ì†Œë¥¼ ë…¼ë¦¬ì  ì—­í•  ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•œë‹¤.
- ë¬¸ì œ ì •ì˜ê°€ ëˆ„êµ¬ì—ê²Œ, ì™œ, ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ êµ¬ì²´ì ì¸ê°€
- ë¬¸ì œ â†’ ì†”ë£¨ì…˜ ì—°ê²°ì´ ê¸°ëŠ¥ ë‚˜ì—´ì´ ì•„ë‹Œ í•´ê²° ë©”ì»¤ë‹ˆì¦˜ì¸ê°€
- ì£¼ì¥ â†’ ê·¼ê±° â†’ ê²°ë¡ ì´ 1:1ë¡œ ì—°ê²°ë˜ëŠ”ê°€
- ë…¼ë¦¬ì  ë¹„ì•½ì´ ì¡´ì¬í•˜ëŠ”ê°€, ìˆë‹¤ë©´ ì¸ì‹Â·ê´€ë¦¬ë˜ëŠ”ê°€
- ìŠ¤í† ë¦¬ íë¦„ì´ ì¼ê´€ì ì¸ê°€ (Problem â†’ Solution â†’ Market â†’ BM â†’ Growth)
- íˆ¬ìì ì§ˆë¬¸(Why now / Why you / Why this way)ì„ ì„ ì œì ìœ¼ë¡œ ë‹µí•˜ëŠ”ê°€
- í•µì‹¬ ë©”ì‹œì§€ê°€ ì‘ì§‘ë˜ì–´ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ ê°€ëŠ¥í•œê°€

---

## [STAGE 2] ì‚°ì—… Ã— íˆ¬ìë‹¨ê³„ Ã— ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ ì í•©ì„± í‰ê°€ (RELATIVE / BONUS)

STAGE 1ì„ í†µê³¼í•œ ê¸°ì—…ë§Œ ìˆ˜í–‰í•œë‹¤.

- íˆ¬ìë‹¨ê³„ ì í•©ì„±: 0â€“10
- ì‚°ì—… ì í•©ì„±: 0â€“10
- ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ ì í•©ì„±: 0â€“10
- ì´ì : 0â€“30
- ê¸°ì¤€ì (í‰ê· ): 5ì 


---

### STAGE 2 ê³µí†µ ì ìˆ˜ í•´ì„
- 8â€“10ì : ëª…í™•íˆ ìš°ìˆ˜ (ë²¤ì¹˜ë§ˆí¬ ìƒíšŒ Hard Data)
- 5â€“7ì : í‰ê·  ìˆ˜ì¤€ (ê°€ì„¤ì€ í•©ë¦¬ì ì´ë‚˜ ê²€ì¦ ì‹œê³„ì—´ ë¶€ì¡±)
- 0â€“4ì : ë¯¸ë‹¬ (í•´ë‹¹ ì¡°ê±´ì—ì„œ ë‹¹ì—°íˆ ìˆì–´ì•¼ í•  ì¦ê±° ëˆ„ë½)

---

### [A] íˆ¬ì ë‹¨ê³„ë³„ ê¸°ëŒ€ ì¦ê±°

#### Seed / Pre-Seed
í•µì‹¬ ì§ˆë¬¸:
â€œê·¼ê±° ì—†ëŠ” ìì‹ ê°ì¸ê°€, ì•„ë‹ˆë©´ ëˆì´ ë˜ëŠ” ë¹„ë°€(Earned Secret)ì„ ì•Œê³  ìˆëŠ”ê°€?â€

í•„ìˆ˜ ì¦ê±°(ì—†ìœ¼ë©´ 3ì  ì´í•˜):
- Earned Secret (í˜„ì¥ì—ì„œë§Œ ì–»ì€ ë¬¸ì œ ì¸ì‚¬ì´íŠ¸)
- Founder-Market Fit
- ì†Œìˆ˜ë¼ë„ ì—´ê´‘í•˜ëŠ” ì´ˆê¸° ì‚¬ìš©ì ì‹ í˜¸

---

#### Series A
í•µì‹¬ ì§ˆë¬¸:
â€œë§ˆì¼€íŒ…ë¹„ë¡œ ë§Œë“  ê°€ì§œ ì„±ì¥ì´ ì•„ë‹Œê°€?â€

í•„ìˆ˜ ì¦ê±°(ì—†ìœ¼ë©´ 3ì  ì´í•˜):
- LTV/CAC â‰¥ 3
- ì½”í˜¸íŠ¸ ê¸°ë°˜ ë¦¬í…ì…˜
- GTM íš¨ìœ¨ì˜ ì‹œê³„ì—´ ê°œì„ 

---

#### Series B+
í•µì‹¬ ì§ˆë¬¸:
â€œê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ì´ìµë„ ì»¤ì§€ëŠ”ê°€?â€

í•„ìˆ˜ ì¦ê±°(ì—†ìœ¼ë©´ 3ì  ì´í•˜):
- NRR â‰¥ 110%
- ìš´ì˜ ë ˆë²„ë¦¬ì§€ ì¡´ì¬
- êµ¬ì¡°ì  ëª¨íŠ¸

---

### [B] ì‚°ì—…ë³„ ë³´ìˆ˜ì  ì£ëŒ€

#### SaaS / ê¸°ìˆ  / í”Œë«í¼
- Churn < 3%
- CAC Payback < 8~12ê°œì›”
- ìì²´ ë°ì´í„°/ì—”ì§„ ì—¬ë¶€

#### ì»¤ë¨¸ìŠ¤ / ë§ˆì¼“í”Œë ˆì´ìŠ¤
- CM2 í‘ì ì—¬ë¶€
- 3ê°œì›” ì¬êµ¬ë§¤ìœ¨ ì—…ê³„ í‰ê·  ëŒ€ë¹„ 1.5ë°°

#### ë°”ì´ì˜¤ / í—¬ìŠ¤ì¼€ì–´ / ë”¥í…Œí¬
- ê·œì œ/ê¸‰ì—¬ ë¡œë“œë§µ ëª…í™•ì„±
- ë¹„êµ ì„ìƒ/ì‹¤í—˜ ë°ì´í„°

---

### [C] ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ë³„ í•µì‹¬ íŒë‹¨
- êµ¬ë…í˜•: ë¦¬í…ì…˜, NRR, ë‹¨ìœ„ê²½ì œì„±
- ê±°ë˜í˜•: GMV Ã— ë¹ˆë„ Ã— ë§ˆì§„
- ê´‘ê³ í˜•: ì°¸ì—¬ë„, ARPU, ë„¤íŠ¸ì›Œí¬ íš¨ê³¼
- ë¼ì´ì„ ìŠ¤: ê³„ì•½ êµ¬ì¡°, ë§ˆì¼ìŠ¤í†¤
- í•˜ë“œì›¨ì–´: ì›ê°€, ë§ˆì§„, ìŠ¤ì¼€ì¼ êµ¬ì¡°
"""


def normalize_folder_id(value: str) -> str:
    text = (value or "").strip()
    if "/folders/" in text:
        return text.split("/folders/", 1)[1].split("?", 1)[0].split("/", 1)[0]
    if "id=" in text:
        return text.split("id=", 1)[1].split("&", 1)[0]
    return text


def status_badge(status: str) -> str:
    mapping = {
        STATUS_DONE: "âœ…ì™„ë£Œ",
        STATUS_PENDING: "ğŸ•’ëŒ€ê¸°",
        STATUS_FAILED: "âš ï¸ì‹¤íŒ¨",
        STATUS_RUNNING: "ğŸ”„ì§„í–‰ì¤‘",
        STATUS_SKIPPED: "âœ…ì™„ë£Œ",
    }
    return mapping.get(status, status or "-")


def short_text(text: str, limit: int = 120) -> str:
    value = (text or "").strip()
    if len(value) <= limit:
        return value
    return value[:limit].rstrip() + "..."


def load_credentials() -> service_account.Credentials:
    import json
    import streamlit as st
    from google.oauth2 import service_account

    # 1) Preferred sectioned secrets
    info = None
    if "google" in st.secrets and "service_account_json" in st.secrets["google"]:
        info = st.secrets["google"]["service_account_json"]
    # 2) Legacy top-level
    elif "service_account_json" in st.secrets:
        info = st.secrets["service_account_json"]
    # 3) Legacy dict fields
    elif "gcp_service_account" in st.secrets:
        info = dict(st.secrets["gcp_service_account"])

    if info is None:
        raise RuntimeError("Missing service_account_json in Streamlit secrets")

    # dict -> use directly
    if isinstance(info, dict):
        sa_info = info
    elif isinstance(info, str):
        s = info.strip()
        # remove one extra wrapping quote layer if present
        if (s.startswith('"') and s.endswith('"')) or (s.startswith("'") and s.endswith("'")):
            s = s[1:-1].strip()
        try:
            sa_info = json.loads(s)
        except Exception as e:
            # safe diagnostics (no secret leak)
            starts = s.lstrip().startswith("{")
            ends = s.rstrip().endswith("}")
            length = len(s)
            raise RuntimeError(
                f"Invalid service_account_json JSON in Streamlit secrets "
                f"(starts_with_{{={starts}}}, ends_with_}}={ends}, length={length})"
            ) from e
    else:
        raise RuntimeError(f"Unsupported service_account_json type: {type(info)}")

    creds = service_account.Credentials.from_service_account_info(
        sa_info,
        scopes=[
            "https://www.googleapis.com/auth/drive",
            "https://www.googleapis.com/auth/spreadsheets",
        ],
    )
    return creds


def get_api_key() -> str:
    api_key = None
    if st.secrets.get("gemini") and st.secrets["gemini"].get("api_key"):
        api_key = st.secrets["gemini"]["api_key"]
    elif st.secrets.get("gemini_api_key"):
        api_key = st.secrets["gemini_api_key"]
    elif st.secrets.get("gemini") and st.secrets["gemini"].get("GEMINI_API_KEY"):
        api_key = st.secrets["gemini"]["GEMINI_API_KEY"]

    if not api_key:
        raise RuntimeError("Missing gemini api key in Streamlit secrets")
    return api_key


def kst_now() -> str:
    kst = tz.gettz("Asia/Seoul")
    return datetime.now(tz=kst).strftime("%Y-%m-%d %H:%M:%S")


def compute_cache_key(
    file_id: str,
    content: str,
    modified_time: str,
    step1_hash: str,
    step2_hash: str,
) -> str:
    parts = [file_id, md5_text(content), modified_time, step1_hash, step2_hash, MODEL_NAME]
    return hash_cache_key(parts)


def ensure_results_folder(drive: DriveClient, source_folder_id: str) -> str:
    drive_id = drive.get_drive_id(source_folder_id)
    return drive.get_or_create_folder(RESULTS_FOLDER_NAME, parent_id=source_folder_id, drive_id=drive_id)


def safe_ensure_results_folder(drive: DriveClient, source_folder_id: str) -> Optional[str]:
    try:
        return ensure_results_folder(drive, source_folder_id)
    except HttpError as exc:
        st.error("í´ë” IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê³µìœ  ë“œë¼ì´ë¸Œ ê¶Œí•œ/IDë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        st.stop()


def ensure_json_folder(drive: DriveClient, results_folder_id: str) -> str:
    drive_id = drive.get_drive_id(results_folder_id)
    return drive.get_or_create_folder(JSON_RESULTS_FOLDER_NAME, parent_id=results_folder_id, drive_id=drive_id)


def compute_final_scores(step1: Dict[str, Any], step2: Optional[Dict[str, Any]]) -> Dict[str, float]:
    logic_score = float(step1.get("logic_score", 0) or 0)
    if step2:
        stage = float(step2.get("stage_score", 0) or 0)
        industry = float(step2.get("industry_score", 0) or 0)
        bm = float(step2.get("bm_score", 0) or 0)
        normalized_step2 = (stage + industry + bm) / 30.0 * 100.0
    else:
        normalized_step2 = 0.0
    final_score = 0.7 * logic_score + 0.3 * normalized_step2
    final_score = max(0.0, min(92.0, final_score))
    return {
        "conservative": round(final_score, 2),
        "neutral": round(final_score, 2),
        "optimistic": round(final_score, 2),
    }


def compute_perspective_scores(step1: Dict[str, Any], step2: Optional[Dict[str, Any]]) -> Dict[str, int]:
    logic_score = float(step1.get("logic_score", 0) or 0)
    if step2:
        stage = float(step2.get("stage_score", 0) or 0)
        industry = float(step2.get("industry_score", 0) or 0)
        bm = float(step2.get("bm_score", 0) or 0)
        normalized_step2 = (stage + industry + bm) / 30.0 * 100.0
    else:
        normalized_step2 = 0.0
    critical = 0.7 * logic_score + 0.3 * normalized_step2
    neutral = 0.6 * logic_score + 0.4 * normalized_step2
    positive = 0.5 * logic_score + 0.5 * normalized_step2
    return {
        "critical": min(92, int(round(critical))),
        "neutral": min(92, int(round(neutral))),
        "positive": min(92, int(round(positive))),
    }


def recommendation_for(score: int) -> str:
    if score >= 80:
        return "ì¶”ì²œ"
    if score >= 70:
        return "ì¡°ê±´ë¶€ ê¶Œì¥"
    return "ë³´ë¥˜"


def derive_recommendations(scores: Dict[str, int]) -> Dict[str, str]:
    return {k: recommendation_for(v) for k, v in scores.items()}


def evaluate_file(
    drive: DriveClient,
    evaluator: Evaluator,
    cache: CacheStore,
    folder_id: str,
    file_meta: Dict[str, Any],
    prompt_step1: str,
    prompt_step2: str,
    step1_hash: str,
    step2_hash: str,
    force_rerun: bool,
) -> Dict[str, Any]:
    file_id = file_meta["id"]
    file_name = file_meta["name"]
    cache_key = ""
    try:
        modified_time = file_meta.get("modifiedTime", "")

        content = _retry(stage="download", func=lambda: drive.get_file_text(file_id))
        cache_key = compute_cache_key(file_id, content, modified_time, step1_hash, step2_hash)
        cached = cache.get(cache_key)
        if cached and not force_rerun:
            return {"status": STATUS_SKIPPED, "file": file_meta, "cache": cached}

        step1_json = _retry(
            stage="step1",
            func=lambda: evaluator.evaluate_step1(
                content=content,
                prompt_step1=f"{prompt_step1}\n\n{PROMPT_APPENDIX}",
                schema_hint_step1=to_json(STEP1_SCHEMA_HINT),
            ),
        )
        logic_score = float(step1_json.get("logic_score", 0) or 0)
        step1_json["pass_gate"] = logic_score >= 80

        step2_json: Optional[Dict[str, Any]] = None
        if step1_json.get("pass_gate", False):
            step2_json = _retry(
                stage="step2",
                func=lambda: evaluator.evaluate_step2(
                    content=content,
                    prompt_step2=f"{prompt_step2}\n\n{PROMPT_APPENDIX}",
                    schema_hint_step2=to_json(STEP2_SCHEMA_HINT),
                    step1_json=step1_json,
                ),
            )

        final_scores = compute_final_scores(step1_json, step2_json)
        perspective_scores = compute_perspective_scores(step1_json, step2_json)
        recommendations = derive_recommendations(perspective_scores)
        final_verdict = recommendations.get("critical", "ë³´ë¥˜")
        report_md = render_report(
            file_name,
            step1_json,
            step2_json,
            perspective_scores,
            recommendations,
            final_verdict,
        )
        report_name = f"{file_name}.report.md"
        report_id = _retry(stage="upload_report", func=lambda: drive.upload_markdown(folder_id, report_name, report_md))
        report_url = _retry(stage="upload_report", func=lambda: drive.get_file_link(report_id))

        json_folder_id = ensure_json_folder(drive, folder_id)
        step1_json_name = f"{file_name}.step1.json"
        step1_json_id = _retry(
            stage="upload_report",
            func=lambda: drive.upload_text(
                json_folder_id, step1_json_name, json.dumps(step1_json, ensure_ascii=True, indent=2), "application/json"
            ),
        )
        step1_json_url = _retry(stage="upload_report", func=lambda: drive.get_file_link(step1_json_id))

        step2_json_id = ""
        step2_json_url = ""
        if step2_json:
            step2_json_name = f"{file_name}.step2.json"
            step2_json_id = _retry(
                stage="upload_report",
                func=lambda: drive.upload_text(
                    json_folder_id,
                    step2_json_name,
                    json.dumps(step2_json, ensure_ascii=True, indent=2),
                    "application/json",
                ),
            )
            step2_json_url = _retry(stage="upload_report", func=lambda: drive.get_file_link(step2_json_id))
        result_payload = {
            "file_id": file_id,
            "file_name": file_name,
            "timestamp": kst_now(),
            "company_name": step1_json.get("company_name", ""),
            "company_description": step1_json.get("one_line_summary", ""),
            "scores": perspective_scores,
            "recommendations": recommendations,
            "final_verdict": final_verdict,
            "overall_summary": step1_json.get("overall_summary", ""),
            "item_evaluations": step1_json.get("item_evaluations", {}),
            "strengths": step1_json.get("strengths", {}),
            "weaknesses": step1_json.get("weaknesses", {}),
            "red_flags": step1_json.get("red_flags", []),
            "axis_scores": {
                "stage": step2_json.get("stage_score") if step2_json else "",
                "industry": step2_json.get("industry_score") if step2_json else "",
                "bm": step2_json.get("bm_score") if step2_json else "",
            },
            "axis_comments": step2_json.get("axis_comments") if step2_json else {},
            "validation_questions": step2_json.get("validation_questions") if step2_json else {},
            "step1_json": step1_json,
            "step2_json": step2_json,
        }
        result_json_name = f"{file_name}.result.json"
        result_json_id = _retry(
            stage="upload_report",
            func=lambda: drive.upload_text(
                json_folder_id,
                result_json_name,
                json.dumps(result_payload, ensure_ascii=True, indent=2),
                "application/json",
            ),
        )
        result_json_url = _retry(stage="upload_report", func=lambda: drive.get_file_link(result_json_id))

        cache_entry = {
            "file_id": file_id,
            "file_name": file_name,
            "source_folder": folder_id,
            "report_file_id": report_id,
            "report_file_url": report_url,
            "step1_json_file_id": step1_json_id,
            "step1_json_file_url": step1_json_url,
            "step2_json_file_id": step2_json_id,
            "step2_json_file_url": step2_json_url,
            "result_json_file_id": result_json_id,
            "result_json_file_url": result_json_url,
            "timestamp": kst_now(),
            "summary": step1_json.get("one_line_summary", ""),
            "step1": step1_json,
            "step2": step2_json,
            "final_scores": final_scores,
            "perspective_scores": perspective_scores,
            "recommendations": recommendations,
            "final_verdict": final_verdict,
        }
        _retry(stage="save_cache", func=lambda: cache.set(cache_key, cache_entry))

        return {
            "status": STATUS_DONE,
            "file": file_meta,
            "cache": cache_entry,
            "report_md": report_md,
        }
    except Exception as exc:
        err_info = format_error_info(exc, file_id, file_name)
        fail_entry = {
            "file_id": file_id,
            "file_name": file_name,
            "source_folder": folder_id,
            "timestamp": kst_now(),
            "status": STATUS_FAILED,
            "error": err_info,
        }
        if cache_key:
            cache.set(cache_key, fail_entry)
        return {"status": STATUS_FAILED, "file": file_meta, "error": err_info}


def _retry(stage: str, func, retries: int = 2) -> Any:
    last_exc: Optional[Exception] = None
    for _ in range(retries + 1):
        try:
            return func()
        except Exception as exc:
            last_exc = exc
            time.sleep(0.6)
    if last_exc:
        raise wrap_stage_error(stage, last_exc) from last_exc
    raise RuntimeError("Unknown error")


def wrap_stage_error(stage: str, exc: Exception) -> Exception:
    return RuntimeError(f"stage={stage} | {exc}")


def format_error_info(exc: Exception, file_id: str, file_name: str) -> Dict[str, str]:
    message = str(exc).replace("\n", " ")[:300]
    return {
        "type": exc.__class__.__name__,
        "message": message,
        "file_id": file_id,
        "file_name": file_name,
    }


def build_sheet_row(cache_entry: Dict[str, Any], source_folder_id: str) -> Dict[str, Any]:
    step1 = cache_entry.get("step1", {})
    perspective_scores = cache_entry.get("perspective_scores", {})
    recommendations = cache_entry.get("recommendations", {})
    step2 = cache_entry.get("step2", {})
    return {
        "timestamp(KST)": cache_entry.get("timestamp", kst_now()),
        "file_id": cache_entry.get("file_id", ""),
        "file_name": cache_entry.get("file_name", ""),
        "source_folder": source_folder_id,
        "company_name": step1.get("company_name", ""),
        "company_description": step1.get("one_line_summary", ""),
        "score_critical": perspective_scores.get("critical", ""),
        "score_neutral": perspective_scores.get("neutral", ""),
        "score_positive": perspective_scores.get("positive", ""),
        "recommendation_critical": recommendations.get("critical", ""),
        "recommendation_neutral": recommendations.get("neutral", ""),
        "recommendation_positive": recommendations.get("positive", ""),
        "overall_summary": step1.get("overall_summary", ""),
        "item_evaluations_json": json.dumps(step1.get("item_evaluations", {}), ensure_ascii=True),
        "strengths_json": json.dumps(step1.get("strengths", {}), ensure_ascii=True),
        "weaknesses_json": json.dumps(step1.get("weaknesses", {}), ensure_ascii=True),
        "red_flags_json": json.dumps(step1.get("red_flags", []), ensure_ascii=True),
        "axis_scores_json": json.dumps(
            {
                "stage": step2.get("stage_score", "") if isinstance(step2, dict) else "",
                "industry": step2.get("industry_score", "") if isinstance(step2, dict) else "",
                "bm": step2.get("bm_score", "") if isinstance(step2, dict) else "",
            },
            ensure_ascii=True,
        ),
        "axis_comments_json": json.dumps(step2.get("axis_comments", {}) if isinstance(step2, dict) else {}, ensure_ascii=True),
        "validation_questions_json": json.dumps(
            step2.get("validation_questions", {}) if isinstance(step2, dict) else {}, ensure_ascii=True
        ),
        "final_verdict": cache_entry.get("final_verdict", ""),
        "report_file_url": cache_entry.get("report_file_url", ""),
        "result_json_url": cache_entry.get("result_json_file_url", ""),
    }


def cache_to_excel_bytes(cache: CacheStore, source_folder_id: str) -> bytes:
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "IR_EVAL"
    ws.append(SHEET_COLUMNS)
    for entry in cache.data.get("items", {}).values():
        row = build_sheet_row(entry, source_folder_id)
        ws.append([row.get(col, "") for col in SHEET_COLUMNS])
    buffer = BytesIO()
    wb.save(buffer)
    return buffer.getvalue()


def excel_filename(source_folder_id: str) -> str:
    stamp = datetime.now(tz=tz.UTC).strftime("%Y%m%d_%H%M")
    return f"ir_eval_{source_folder_id}_{stamp}.xlsx"


def get_report_text(drive: DriveClient, entry: Dict[str, Any]) -> str:
    if entry.get("report_md"):
        return entry["report_md"]
    report_id = entry.get("report_file_id")
    if report_id:
        return drive.get_file_text(report_id)
    return ""


def render_results_list(drive: DriveClient, cache: CacheStore, folder_id: str) -> None:
    items = list(cache.data.get("items", {}).values())
    if not items:
        st.info("íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    st.subheader("ê²°ê³¼ ëª©ë¡")
    items_sorted = sorted(items, key=lambda x: x.get("timestamp", ""), reverse=True)
    result_rows = []
    result_label_map = {}
    for entry in items_sorted:
        name = entry.get("file_name", "")
        entry_id = entry.get("file_id", "")
        scores = entry.get("perspective_scores", {})
        recs = entry.get("recommendations", {})
        result_rows.append(
            {
                "file_name": name,
                "timestamp": entry.get("timestamp", ""),
                "critical": scores.get("critical", ""),
                "neutral": scores.get("neutral", ""),
                "positive": scores.get("positive", ""),
                "recommendation": recs.get("critical", ""),
                "report_url": entry.get("report_file_url", ""),
            }
        )
        result_label_map[f"{name} [{entry_id[:6]}]"] = entry

    st.dataframe(result_rows, use_container_width=True, height=320)
    selected_result = st.selectbox("ê²°ê³¼ ì„ íƒ", list(result_label_map.keys()))
    entry = result_label_map.get(selected_result)
    if entry:
        cols = st.columns([2, 2, 2, 6])
        if cols[0].button("ê²°ê³¼ë³´ê¸°"):
            st.session_state["last_report"] = get_report_text(drive, entry)
        report_text = get_report_text(drive, entry)
        cols[1].download_button(
            label="ë‹¤ìš´ë¡œë“œ",
            data=report_text or "",
            file_name=f"{entry.get('file_name','')}.report.md",
            mime="text/markdown",
        )
        result_json_id = entry.get("result_json_file_id", "")
        if result_json_id:
            result_json_text = drive.get_file_text(result_json_id)
            cols[2].download_button(
                label="JSON",
                data=result_json_text,
                file_name=f"{entry.get('file_name','')}.result.json",
                mime="application/json",
            )
        if entry.get("report_file_url"):
            cols[3].markdown(f"[ë¦¬í¬íŠ¸ ì—´ê¸°]({entry['report_file_url']})")

    excel_bytes = cache_to_excel_bytes(cache, folder_id)
    st.download_button(
        label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
        data=excel_bytes,
        file_name=excel_filename(folder_id),
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )


def init_session_state() -> None:
    st.session_state.setdefault("folder_id", "")
    st.session_state.setdefault("files", [])
    st.session_state.setdefault("results", [])
    st.session_state.setdefault("selected_file_id", "")
    st.session_state.setdefault("selected_file_name", "")
    st.session_state.setdefault("selected_file_ids", [])
    st.session_state.setdefault("last_report", "")
    st.session_state.setdefault("status_map", {})
    st.session_state.setdefault("rerun_file_id", "")
    st.session_state.setdefault("page", 1)


def render_sidebar(drive: DriveClient) -> Dict[str, Any]:
    st.sidebar.header("#ì‚¬ì´ë“œë°”")
    folder_input = st.sidebar.text_input("Google drive í´ë” ID", value=st.session_state.get("folder_id", ""))
    folder_id = normalize_folder_id(folder_input)
    st.session_state["folder_id"] = folder_id

    action_cols = st.sidebar.columns([1, 1, 1])
    scan_clicked = action_cols[0].button("í´ë” ìŠ¤ìº”")
    refresh_clicked = action_cols[1].button("ìºì‹œ ìƒˆë¡œê³ ì¹¨")
    delete_cache_clicked = action_cols[2].button("ìºì‹œ ì‚­ì œ")
    if refresh_clicked and folder_id:
        st.session_state["cache_reload"] = True
    if delete_cache_clicked and folder_id:
        st.session_state["cache_delete"] = True

    st.sidebar.subheader("íŒŒì¼ ëª©ë¡ ë¦¬ìŠ¤íŠ¸")
    files = st.session_state.get("files", [])
    file_rows = []
    file_map = {}
    for f in files:
        status = status_badge(st.session_state["status_map"].get(f["id"], STATUS_PENDING))
        file_rows.append({"íŒŒì¼ëª…": f["name"], "ì§„í–‰": status})
        file_map[f"{f['name']} [{f['id'][:6]}]"] = f["id"]
    st.sidebar.dataframe(file_rows, use_container_width=True, height=240)

    if file_map:
        labels = list(file_map.keys())
        st.sidebar.markdown("í‰ê°€ ëŒ€ìƒ ì„ íƒ")
        selected_ids = set(st.session_state.get("selected_file_ids", []))
        checkbox_box = st.sidebar.container()
        new_selected_ids = []
        with checkbox_box:
            for label in labels:
                checked = st.checkbox(
                    short_text(label, 36),
                    value=file_map[label] in selected_ids,
                    key=f"select_{file_map[label]}",
                )
                if checked:
                    new_selected_ids.append(file_map[label])
        st.session_state["selected_file_ids"] = new_selected_ids

        default_index = 0
        current_id = st.session_state.get("selected_file_id")
        if current_id:
            for idx, label in enumerate(labels):
                if file_map[label] == current_id:
                    default_index = idx
                    break
        selected_label = st.sidebar.selectbox("ë¯¸ë¦¬ë³´ê¸° ì„ íƒ", labels, index=default_index)
        st.session_state["selected_file_id"] = file_map.get(selected_label, "")
        st.session_state["selected_file_name"] = selected_label.split(" [", 1)[0] if selected_label else ""

    st.sidebar.subheader("í‰ê°€ ì‹¤í–‰")
    force_rerun = st.sidebar.checkbox("ìºì‹œ ë¬´ì‹œ(ì¬í‰ê°€)", value=False)
    btn_cols = st.sidebar.columns(3)
    evaluate_selected = btn_cols[0].button("ì„ íƒ í‰ê°€")
    evaluate_all = btn_cols[1].button("ì „ì²´ í‰ê°€")
    load_history = btn_cols[2].button("íˆìŠ¤í† ë¦¬")

    return {
        "folder_id": folder_id,
        "scan_clicked": scan_clicked,
        "force_rerun": force_rerun,
        "evaluate_selected": evaluate_selected,
        "evaluate_all": evaluate_all,
        "load_history": load_history,
        "delete_cache_clicked": delete_cache_clicked,
    }


def render_main_header(cache: Optional[CacheStore], folder_id: str) -> None:
    cols = st.columns([3, 1])
    cols[0].header("í‰ê°€ ë¦¬í¬íŠ¸")
    if cache:
        excel_bytes = cache_to_excel_bytes(cache, folder_id)
        cols[1].download_button(
            label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=excel_bytes,
            file_name=excel_filename(folder_id),
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    else:
        cols[1].button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", disabled=True)


def render_report_table(drive: DriveClient, cache: Optional[CacheStore]) -> None:
    if not cache:
        st.info("ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    items = list(cache.data.get("items", {}).values())
    if not items:
        st.info("ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    rows = []
    entry_map = {}
    for entry in sorted(items, key=lambda x: x.get("timestamp", ""), reverse=True):
        step1 = entry.get("step1", {})
        scores = entry.get("perspective_scores", {})
        file_name = entry.get("file_name", "")
        entry_map[file_name] = entry
        rows.append(
            {
                "íŒŒì¼ëª…": file_name,
                "ê¸°ì—…ëª…": step1.get("company_name", ""),
                "critical": scores.get("critical", ""),
                "neutral": scores.get("neutral", ""),
                "positive": scores.get("positive", ""),
                "ë¯¸ë¦¬ë³´ê¸°": "ë³´ê¸°",
                ".md ë‹¤ìš´ë¡œë“œ": "ë‹¤ìš´ë¡œë“œ",
            }
        )

    st.dataframe(rows, use_container_width=True, height=260)
    selected_name = st.selectbox("ë¦¬í¬íŠ¸ ì„ íƒ", list(entry_map.keys()))
    entry = entry_map.get(selected_name)
    action_cols = st.columns([1, 1, 2, 4])
    if action_cols[0].button("ë³´ê¸°"):
        st.session_state["selected_file_id"] = entry.get("file_id", "")
        st.session_state["selected_file_name"] = entry.get("file_name", "")
        st.session_state["last_report"] = get_report_text(drive, entry)
    report_text = get_report_text(drive, entry)
    action_cols[1].download_button(
        label=".md ë‹¤ìš´ë¡œë“œ",
        data=report_text or "",
        file_name=f"{entry.get('file_name','')}.report.md",
        mime="text/markdown",
    )
    if entry.get("report_file_url"):
        action_cols[2].markdown(f"[ë¦¬í¬íŠ¸ ì—´ê¸°]({entry['report_file_url']})")
    if entry.get("result_json_file_id"):
        result_json_text = drive.get_file_text(entry["result_json_file_id"])
        action_cols[3].download_button(
            label="JSON ë‹¤ìš´ë¡œë“œ",
            data=result_json_text,
            file_name=f"{entry.get('file_name','')}.result.json",
            mime="application/json",
        )


def find_selected_entry(cache: Optional[CacheStore]) -> Optional[Dict[str, Any]]:
    if not cache:
        return None
    selected_id = st.session_state.get("selected_file_id")
    selected_name = st.session_state.get("selected_file_name")
    for entry in cache.data.get("items", {}).values():
        if selected_id and entry.get("file_id") == selected_id:
            return entry
        if selected_name and entry.get("file_name") == selected_name:
            return entry
    return None


def render_preview_panel(entry: Optional[Dict[str, Any]]) -> None:
    st.subheader("ë¯¸ë¦¬ë³´ê¸°")
    if not entry:
        st.info("ì„ íƒëœ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    step1 = entry.get("step1", {})
    scores = entry.get("perspective_scores", {})
    company_name = step1.get("company_name") or "ê¸°ì—…ëª… ë¯¸ìƒ"
    title = f"{company_name} ë¶„ì„ ê²°ê³¼"
    st.markdown(
        f"#ë¦¬í¬íŠ¸ ì œëª©  {title}  \n"
        f"Critical : {scores.get('critical','')}   "
        f"Neutral : {scores.get('neutral','')}   "
        f"Positive : {scores.get('positive','')}"
    )
    st.markdown(step1.get("one_line_summary", ""))

    st.markdown("### Title : ì¢…í•© í‰ê°€")
    st.info(step1.get("overall_summary", "(ì—†ìŒ)"))

    item_evaluations = step1.get("item_evaluations", {})
    if not item_evaluations:
        st.info("í•­ëª©ë³„ í‰ê°€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    st.markdown("### í•­ëª©ë³„ í‰ê°€")
    short_items = []
    for i in range(0, len(ITEM_KEYS), 2):
        cols = st.columns(2)
        for j, key in enumerate(ITEM_KEYS[i : i + 2]):
            value = item_evaluations.get(key, {})
            comment = value.get("comment", "")
            feedback = value.get("feedback", "")
            cols[j].markdown(f"**Title : {key}**")
            cols[j].write(comment or "(ì½”ë©˜íŠ¸ ì—†ìŒ)")
            cols[j].write(feedback or "(í”¼ë“œë°± ì—†ìŒ)")
            if len((comment + feedback).strip()) < 80 or len((comment + feedback).strip()) > 120:
                cols[j].caption("ê¶Œì¥ ë¶„ëŸ‰: 80~120ì")
            if len(value.get("comment", "")) < 200 or len(value.get("feedback", "")) < 200:
                short_items.append(key)
    if short_items:
        st.warning(f"200ì ë¯¸ë§Œ í•­ëª©: {', '.join(short_items)}")


def main() -> None:
    st.set_page_config(page_title="IR Evaluator", layout="wide")

    try:
        credentials = load_credentials()
        api_key = get_api_key()
    except RuntimeError as exc:
        st.error(str(exc))
        st.stop()

    drive = DriveClient(credentials)

    init_session_state()

    st.title("Title : IR ë¶„ì„ & í‰ê°€")

    top_cols = st.columns([4, 1, 1, 1, 1], gap="small")
    folder_input = top_cols[0].text_input(
        "Google drive í´ë” ID",
        value=st.session_state.get("folder_id", ""),
        placeholder="í´ë” ID ë˜ëŠ” URL",
    )
    folder_id = normalize_folder_id(folder_input)
    st.session_state["folder_id"] = folder_id

    scan_clicked = top_cols[1].button("ë¬¸ì„œ ìŠ¤ìº”")
    force_rerun = top_cols[2].checkbox("ìºì‹œ ë¬´ì‹œ(ì¬í‰ê°€)", value=False)
    refresh_clicked = top_cols[3].button("ìºì‹œ ìƒˆë¡œê³ ì¹¨")
    delete_cache_clicked = top_cols[4].button("ìºì‹œ ì‚­ì œ")
    cache = None
    result_folder_id = ""
    if refresh_clicked and folder_id:
        result_folder_id = safe_ensure_results_folder(drive, folder_id)
        if result_folder_id:
            cache = CacheStore(drive, result_folder_id)
            cache.load()

    if delete_cache_clicked and folder_id:
        result_folder_id = safe_ensure_results_folder(drive, folder_id)
        if result_folder_id:
            existing = drive.find_file_in_folder(result_folder_id, "cache_index.json", mime_type="application/json")
            if existing:
                drive.service.files().delete(fileId=existing["id"], supportsAllDrives=True).execute()
            cache = CacheStore(drive, result_folder_id)
            cache.load()

    if scan_clicked and folder_id:
        result_folder_id = safe_ensure_results_folder(drive, folder_id)
        if result_folder_id:
            cache = CacheStore(drive, result_folder_id)
            cache.load()
        with st.spinner("ìŠ¤ìº” ì¤‘..."):
            st.session_state["files"] = drive.list_md_files(folder_id)
            st.session_state["status_map"] = {f["id"]: STATUS_PENDING for f in st.session_state["files"]}

    files = st.session_state.get("files", [])
    if not files:
        st.info("í´ë”ë¥¼ ìŠ¤ìº”í•˜ë©´ .md íŒŒì¼ ëª©ë¡ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
        return

    table_header = st.columns([3, 1], gap="small")
    table_header[0].subheader("íŒŒì¼ ëª©ë¡ & IR List")
    if cache:
        excel_bytes = cache_to_excel_bytes(cache, folder_id)
        table_header[1].download_button(
            label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=excel_bytes,
            file_name=excel_filename(folder_id),
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    else:
        table_header[1].button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", disabled=True)

    search_term = st.text_input("ê²€ìƒ‰(íŒŒì¼ëª…/ê¸°ì—…ëª…)", value="")
    cache_items = {}
    if cache:
        for entry in cache.data.get("items", {}).values():
            cache_items[entry.get("file_id", "")] = entry

    selected_ids = set(st.session_state.get("selected_file_ids", []))
    filtered_files = []
    for f in files:
        entry = cache_items.get(f["id"])
        company_name = entry.get("step1", {}).get("company_name", "") if entry else ""
        if search_term:
            term = search_term.strip().lower()
            if term not in f["name"].lower() and term not in company_name.lower():
                continue
        filtered_files.append(f)

    page_size = 10
    total_pages = max(1, (len(filtered_files) + page_size - 1) // page_size)
    page = min(st.session_state.get("page", 1), total_pages)
    pager_cols = st.columns([1, 1, 2, 1, 1], gap="small")
    if pager_cols[0].button("ì´ì „"):
        page = max(1, page - 1)
    pager_cols[2].markdown(f"í˜ì´ì§€ {page}/{total_pages}")
    if pager_cols[4].button("ë‹¤ìŒ"):
        page = min(total_pages, page + 1)
    st.session_state["page"] = page

    start = (page - 1) * page_size
    end = start + page_size
    for f in filtered_files[start:end]:
        entry = cache_items.get(f["id"])
        company_name = entry.get("step1", {}).get("company_name", "") if entry else ""
        scores = entry.get("perspective_scores", {}) if entry else {}

        row = st.columns([3, 1, 1, 1, 1, 1, 1, 1, 1], gap="small")
        row[0].write(f["name"])
        row[1].write(status_badge(st.session_state["status_map"].get(f["id"], STATUS_PENDING)))
        checked = row[2].checkbox(
            "",
            value=f["id"] in selected_ids,
            key=f"select_{f['id']}",
        )
        if checked:
            selected_ids.add(f["id"])
        else:
            selected_ids.discard(f["id"])
        row[3].write(company_name)
        row[4].write(scores.get("critical", ""))
        row[5].write(scores.get("neutral", ""))
        row[6].write(scores.get("positive", ""))
        if row[7].button("ë³´ê¸°", key=f"preview_{f['id']}") and entry:
            st.session_state["selected_file_id"] = f["id"]
            st.session_state["selected_file_name"] = f["name"]
            st.session_state["last_report"] = get_report_text(drive, entry)
        report_url = entry.get("report_file_url") if entry else ""
        if report_url:
            row[8].markdown(f"[íŒŒì¼ì—´ê¸°]({report_url})")
        else:
            row[8].write("-")

    st.session_state["selected_file_ids"] = list(selected_ids)

    action_cols = st.columns([6, 1, 1, 1], gap="small")
    evaluate_selected = action_cols[1].button("ì„ íƒ í‰ê°€")
    evaluate_all = action_cols[2].button("ì „ì²´ í‰ê°€")
    load_history = action_cols[3].button("íˆìŠ¤í† ë¦¬")

    rerun_file_id = st.session_state.get("rerun_file_id")
    if rerun_file_id:
        evaluate_selected = True
        force_rerun = True
        st.session_state["rerun_file_id"] = ""

    if evaluate_selected or evaluate_all:
        if not result_folder_id:
            result_folder_id = safe_ensure_results_folder(drive, folder_id)
        if result_folder_id and not cache:
            cache = CacheStore(drive, result_folder_id)
            cache.load()
        if evaluate_all:
            target_files = files
        else:
            selected_ids = set(st.session_state.get("selected_file_ids", []))
            target_files = [f for f in files if f["id"] in selected_ids]
        if rerun_file_id:
            target_files = [f for f in files if f["id"] == rerun_file_id]
        if not target_files:
            st.warning("í‰ê°€í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")
            return

        prompt_step1 = BASE_PROMPT
        prompt_step2 = BASE_PROMPT
        step1_hash = hash_prompt(prompt_step1)
        step2_hash = hash_prompt(prompt_step2)

        semaphore = threading.Semaphore(2)
        evaluator = Evaluator(api_key=api_key, semaphore=semaphore)

        results: List[Dict[str, Any]] = []
        progress = st.progress(0)
        progress_text = st.empty()
        completed = 0
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                futures = []
                for f in target_files:
                    st.session_state["status_map"][f["id"]] = STATUS_RUNNING
                    futures.append(
                        executor.submit(
                            evaluate_file,
                            drive,
                            evaluator,
                            cache,
                            result_folder_id,
                            f,
                            prompt_step1,
                            prompt_step2,
                            step1_hash,
                            step2_hash,
                            force_rerun,
                        )
                    )
                for future in concurrent.futures.as_completed(futures):
                    try:
                        results.append(future.result())
                    except Exception as exc:
                        results.append(
                            {
                                "status": STATUS_FAILED,
                                "file": {"id": "", "name": ""},
                                "error": format_error_info(exc, "", ""),
                            }
                        )
                    completed += 1
                    progress.progress(completed / len(target_files))
                    progress_text.write(f"ì§„í–‰: {completed}/{len(target_files)}")
        finally:
            if cache:
                cache.save()

        failed = []
        for res in results:
            status = res.get("status")
            file_meta = res.get("file", {})
            file_id = file_meta.get("id", "")
            file_name = file_meta.get("name", "")
            if file_id:
                st.session_state["status_map"][file_id] = status
            if status == STATUS_FAILED and res.get("error"):
                failed.append(res["error"])
            cache_entry = res.get("cache", {})
            if res.get("report_md"):
                st.session_state["last_report"] = res["report_md"]
                st.session_state["selected_file_id"] = cache_entry.get("file_id", file_id)
                st.session_state["selected_file_name"] = cache_entry.get("file_name", file_name)

        if failed:
            st.subheader("ì‹¤íŒ¨ ìƒì„¸")
            for item in failed:
                st.write(
                    f"{item.get('type')} | {item.get('message')} | "
                    f"file_id={item.get('file_id')} | file_name={item.get('file_name')}"
                )

    if load_history and folder_id:
        pass

    selected_entry = find_selected_entry(cache)
    render_preview_panel(selected_entry)


if __name__ == "__main__":
    main()
