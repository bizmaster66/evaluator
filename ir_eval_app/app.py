from __future__ import annotations

import concurrent.futures
import hashlib
import json
import threading
import time
from datetime import datetime
from io import BytesIO
from typing import Any, Dict, List, Optional

import openpyxl
import streamlit as st
from dateutil import tz

from src.config import MODEL_NAME, hash_prompt, md5_text, to_json
from src.evaluator import Evaluator
from src.report_writer import render_report

SCOPES = []

STEP1_SCHEMA_HINT = {
    "company_name": "string",
    "one_line_summary": "string",
    "overall_summary": "string (ì¢…í•© í‰ê°€ ìš”ì•½)",
    "logic_score": "number 0-100",
    "pass_gate": "boolean (logic_score >= 80)",
    "item_evaluations": {
        "ë¬¸ì œì •ì˜": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì†”ë£¨ì…˜&ì œí’ˆ": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì‹œì¥ê·œëª¨&ë¶„ì„": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ê²½ìŸë¶„ì„": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì„±ì¥ì „ëµ": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì£¼ìš” ì¸ë ¥&íŒ€": {"score": "number 0-10", "comment": "string", "feedback": "string"},
        "ì¬ë¬´ê³„íš": {"score": "number 0-10", "comment": "string", "feedback": "string"},
    },
    "item_scores": {"market": "number 0-10", "team": "number 0-10", "product": "number 0-10"},
    "strengths": {"market": "list[str]", "team": "list[str]", "product": "list[str]"},
    "weaknesses": {"market": "list[str]", "team": "list[str]", "product": "list[str]"},
    "red_flags": "list[str]",
    "cost_estimate": {"llm_calls": "number", "tokens": "number", "usd": "number"},
}

STEP2_SCHEMA_HINT = {
    "stage_score": "number 0-10",
    "industry_score": "number 0-10",
    "bm_score": "number 0-10",
    "axis_comments": {"stage": "string", "industry": "string", "bm": "string"},
    "validation_questions": {"stage": "list[str]", "industry": "list[str]", "bm": "list[str]"},
    "cost_estimate": {"llm_calls": "number", "tokens": "number", "usd": "number"},
}

SHEET_COLUMNS = [
    "timestamp(KST)",
    "file_name",
    "company_name",
    "company_description",
    "score_critical",
    "score_neutral",
    "score_positive",
    "recommendation_critical",
    "recommendation_neutral",
    "recommendation_positive",
    "overall_summary",
    "item_evaluations_json",
    "strengths_json",
    "weaknesses_json",
    "red_flags_json",
    "axis_scores_json",
    "axis_comments_json",
    "validation_questions_json",
    "final_verdict",
]

STATUS_PENDING = "ëŒ€ê¸°"
STATUS_RUNNING = "ì§„í–‰ì¤‘"
STATUS_SKIPPED = "ìŠ¤í‚µ"
STATUS_DONE = "ì™„ë£Œ"
STATUS_FAILED = "ì‹¤íŒ¨"

ITEM_KEYS = [
    "ë¬¸ì œì •ì˜",
    "ì†”ë£¨ì…˜&ì œí’ˆ",
    "ì‹œì¥ê·œëª¨&ë¶„ì„",
    "ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸",
    "ê²½ìŸë¶„ì„",
    "ì„±ì¥ì „ëµ",
    "ì£¼ìš” ì¸ë ¥&íŒ€",
    "ì¬ë¬´ê³„íš",
]

PROMPT_APPENDIX = (
    "ì¶”ê°€ ì§€ì‹œì‚¬í•­:\n"
    "1) Step1/Step2 JSONì€ ë°˜ë“œì‹œ ìŠ¤í‚¤ë§ˆ íŒíŠ¸ì— ë§ì¶° ì¶œë ¥í•œë‹¤.\n"
    "2) í•­ëª©ë³„ í‰ê°€ëŠ” ë‹¤ìŒ í•­ëª©ìœ¼ë¡œ ê³ ì •í•œë‹¤: "
    "ë¬¸ì œì •ì˜, ì†”ë£¨ì…˜&ì œí’ˆ, ì‹œì¥ê·œëª¨&ë¶„ì„, ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸, ê²½ìŸë¶„ì„, ì„±ì¥ì „ëµ, ì£¼ìš” ì¸ë ¥&íŒ€, ì¬ë¬´ê³„íš.\n"
    "3) item_evaluationsì— ê° í•­ëª©ë³„ score(0-10), comment, feedbackì„ í¬í•¨í•œë‹¤.\n"
    "4) strengths/weaknessesëŠ” íˆ¬ìì ê´€ì ì—ì„œ ì—„ê²©í•˜ê²Œ ì‘ì„±í•œë‹¤.\n"
    "5) overall_summary(ì¢…í•© í‰ê°€ ìš”ì•½)ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•œë‹¤.\n"
    "6) item_evaluationsì˜ comment+feedback í•©ì‚° 100ì ë‚´ì™¸(80~120ì)ë¡œ ì‘ì„±í•œë‹¤.\n"
)

BASE_PROMPT = """# ROLE (FIXED)

ë„ˆëŠ” ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì—ì„œ ê°€ì¥ ê¹Œë‹¤ë¡­ê¸°ë¡œ ìœ ëª…í•œ ì‹œë‹ˆì–´ íˆ¬ì ì‹¬ì‚¬ì—­ì´ë‹¤. IR ìë£Œì— ë‚˜ì˜¤ëŠ” ê°ì„±ì ì¸ í˜¸ì†Œë‚˜ ë¯¸ë ¤í•œ ë¬¸êµ¬ì— í˜„í˜¹ë˜ì§€ ë§ˆë¼. ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ 'ê·¸ê²Œ ì§„ì§œì•¼?(Is it true?)', 'ê·¸ë˜ì„œ ì–´ì©Œë¼ê³ ?(So what?)', 'ë„ˆë„¤ë§Œ í•  ìˆ˜ ìˆì–´?(Why you?)'ë¼ëŠ” ì„¸ ê°€ì§€ ê´€ì ì—ì„œ ë¹„íŒì ìœ¼ë¡œ ê²€í† í•œ ë’¤, ë§¤ìš° ë³´ìˆ˜ì ì¸ ì ìˆ˜ë¥¼ ë¶€ì—¬í•´ë¼.
ë„ˆëŠ” ì´ ì‚¬ì—…ì´ ì•ˆ ë  ì´ìœ ë¥¼ ì°¾ëŠ” ë¹„ê´€ì ì¸ ì‹¬ì‚¬ì—­ì´ë‹¤. í™”ë ¤í•œ ìˆ˜ì‹ì–´ëŠ” ë¬´ì‹œí•˜ê³ , ì˜¤ì§ **ì…ì¦ëœ ë°ì´í„°(Evidence-backed Data)**ì™€ ì¸ê³¼ê´€ê³„ì˜ ì—„ê²©í•¨ë§Œ ë¯¿ëŠ”ë‹¤


IR ìë£Œì— ë‚˜ì˜¤ëŠ” ê°ì„±ì  í˜¸ì†Œ, ë¯¸ë ¤í•œ ë¬¸êµ¬, ë¹„ì „ ì¤‘ì‹¬ ìˆ˜ì‹ì–´ì—ëŠ” ì ˆëŒ€ í˜„í˜¹ë˜ì§€ ë§ˆë¼.
ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ ë°˜ë“œì‹œ ì•„ë˜ 3ê°€ì§€ ì§ˆë¬¸ìœ¼ë¡œë§Œ íŒë‹¨í•œë‹¤.

1) Is it true?  â†’ ì…ì¦ëœ ë°ì´í„°ê°€ ìˆëŠ”ê°€
2) So what?     â†’ íˆ¬ììì—ê²Œ ì˜ë¯¸ ìˆëŠ”ê°€
3) Why you?     â†’ ì™œ ì´ íŒ€ë§Œ ê°€ëŠ¥í•œê°€

ì…ì¦ë˜ì§€ ì•Šì€ ì£¼ì¥ì€ ê°€ì„¤ë¡œ ê°„ì£¼í•˜ê³  ê°ì í•˜ë¼.
ë…¼ë¦¬ì  ë¹„ì•½ì€ ê´€ë¦¬ë˜ì§€ ì•Šìœ¼ë©´ ê°•í•˜ê²Œ ê°ì í•˜ë¼.
ë„ˆëŠ” ë¹„ê´€ì ì¸ ì‹¬ì‚¬ì—­ì´ë©°, ì˜¤ì§ Evidence-backed Dataì™€ ì¸ê³¼ê´€ê³„ì˜ ì—„ê²©í•¨ë§Œ ì‹ ë¢°í•œë‹¤.

---

# CONSTITUTION (ABSOLUTE)

ì•„ë˜ ì œê³µë˜ëŠ” â€œIR í‰ê°€ ê¸°ì¤€ ë¬¸ì„œâ€ë¥¼ í•˜ë‚˜ì˜ í—Œë²•ì²˜ëŸ¼ ì ˆëŒ€ì ìœ¼ë¡œ ë”°ë¥¸ë‹¤.
ì„ì˜ë¡œ í•´ì„ì„ í™•ì¥í•˜ê±°ë‚˜ ê¸°ì¤€ì„ ì™„í™”í•˜ì§€ ì•ŠëŠ”ë‹¤.

---

# HARD RULES (NON-NEGOTIABLE)

1. ì¶œë ¥ì€ JSONê³¼ ë§ˆí¬ë‹¤ìš´íŒŒì¼ë¡œ í•˜ê³  ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥í•œë‹¤.
2. JSONì€ ì§€ì •ëœ ìŠ¤í‚¤ë§ˆì™€ ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•œë‹¤.
3. ê°•ì /ì•½ì ì€ ë°˜ë“œì‹œ íˆ¬ìì ê´€ì ì—ì„œ ì‘ì„±í•œë‹¤.
4. ì ìˆ˜ëŠ” ëƒ‰ì •í•˜ê²Œ ë¶€ì—¬í•˜ë©°, ì˜ì‹¬ë˜ëŠ” ì§€ì ë§ˆë‹¤ ê¹ëŠ”ë‹¤.

---

# INPUT SCOPE

- ì…ë ¥: IR full-text Markdown (.md)

---

# OVERALL GOAL

â€œì´ íšŒì‚¬ëŠ” ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ë“ë˜ë©°,
í•´ë‹¹ ì‚°ì—… Ã— íˆ¬ìë‹¨ê³„ Ã— ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ ì¡°ê±´ì—ì„œ
í‰ê·  ëŒ€ë¹„ ìš°ìˆ˜í•œê°€?â€

---

## [STAGE 1] IR ë…¼ë¦¬ì„±Â·ì¶©ì‹¤ì„± í‰ê°€ (GATE / ABSOLUTE)

- ì´ì : 0â€“100
- ì»·íŠ¸ë¼ì¸: **80ì **
- 80ì  ë¯¸ë§Œì´ë©´:
  â†’ ì¦‰ì‹œ ë¯¸íŒ… íŒë‹¨ = NO
  â†’ STAGE 2ëŠ” ìˆ˜í–‰í•˜ì§€ ì•ŠëŠ”ë‹¤.

### STAGE 1 í•µì‹¬ ì² í•™
â€œì´ IRì€ íˆ¬ììë¥¼ ì„¤ë“í•  ë…¼ë¦¬ êµ¬ì¡°ë¥¼ ê°–ì¶”ì—ˆëŠ”ê°€?â€

### ë³´ìˆ˜ì  ê°ì  ê·œì¹™ (ë°˜ë“œì‹œ ì ìš©)
- â€˜í˜ì‹ ì â€™, â€˜ì„¸ê³„ ìµœì´ˆâ€™ ë“± ì¶”ìƒì  í˜•ìš©ì‚¬ ë‚¨ë°œ â†’ ë…¼ë¦¬ ëª¨í˜¸ì„±ìœ¼ë¡œ ê°ì 
- TAMë§Œ í‚¤ìš°ê³  SOM(ì‹¤ì œ í•´ê²° ê°€ëŠ¥ ë²”ìœ„)ì´ ë¶ˆëª…í™• â†’ ê°ì 
- ì£¼ì¥ê³¼ ë°ì´í„°ê°€ 1:1ë¡œ ë§¤ì¹­ë˜ì§€ ì•ŠìŒ â†’ í—ˆìœ„ ë…¼ë¦¬ë¡œ ê°„ì£¼

### STAGE 1 í‰ê°€ ê´€ì 
ë‹¤ìŒ ìš”ì†Œë¥¼ ë…¼ë¦¬ì  ì—­í•  ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•œë‹¤.
- ë¬¸ì œ ì •ì˜ê°€ ëˆ„êµ¬ì—ê²Œ, ì™œ, ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ êµ¬ì²´ì ì¸ê°€
- ë¬¸ì œ â†’ ì†”ë£¨ì…˜ ì—°ê²°ì´ ê¸°ëŠ¥ ë‚˜ì—´ì´ ì•„ë‹Œ í•´ê²° ë©”ì»¤ë‹ˆì¦˜ì¸ê°€
- ì£¼ì¥ â†’ ê·¼ê±° â†’ ê²°ë¡ ì´ 1:1ë¡œ ì—°ê²°ë˜ëŠ”ê°€
- ë…¼ë¦¬ì  ë¹„ì•½ì´ ì¡´ì¬í•˜ëŠ”ê°€, ìˆë‹¤ë©´ ì¸ì‹Â·ê´€ë¦¬ë˜ëŠ”ê°€
- ìŠ¤í† ë¦¬ íë¦„ì´ ì¼ê´€ì ì¸ê°€ (Problem â†’ Solution â†’ Market â†’ BM â†’ Growth)
- íˆ¬ìì ì§ˆë¬¸(Why now / Why you / Why this way)ì„ ì„ ì œì ìœ¼ë¡œ ë‹µí•˜ëŠ”ê°€
- í•µì‹¬ ë©”ì‹œì§€ê°€ ì‘ì§‘ë˜ì–´ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ ê°€ëŠ¥í•œê°€

---

## [STAGE 2] ì‚°ì—… Ã— íˆ¬ìë‹¨ê³„ Ã— ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ ì í•©ì„± í‰ê°€ (RELATIVE / BONUS)

STAGE 1ì„ í†µê³¼í•œ ê¸°ì—…ë§Œ ìˆ˜í–‰í•œë‹¤.

- íˆ¬ìë‹¨ê³„ ì í•©ì„±: 0â€“10
- ì‚°ì—… ì í•©ì„±: 0â€“10
- ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ ì í•©ì„±: 0â€“10
- ì´ì : 0â€“30
- ê¸°ì¤€ì (í‰ê· ): 5ì 


---

### STAGE 2 ê³µí†µ ì ìˆ˜ í•´ì„
- 8â€“10ì : ëª…í™•íˆ ìš°ìˆ˜ (ë²¤ì¹˜ë§ˆí¬ ìƒíšŒ Hard Data)
- 5â€“7ì : í‰ê·  ìˆ˜ì¤€ (ê°€ì„¤ì€ í•©ë¦¬ì ì´ë‚˜ ê²€ì¦ ì‹œê³„ì—´ ë¶€ì¡±)
- 0â€“4ì : ë¯¸ë‹¬ (í•´ë‹¹ ì¡°ê±´ì—ì„œ ë‹¹ì—°íˆ ìˆì–´ì•¼ í•  ì¦ê±° ëˆ„ë½)

---

### [A] íˆ¬ì ë‹¨ê³„ë³„ ê¸°ëŒ€ ì¦ê±°

#### Seed / Pre-Seed
í•µì‹¬ ì§ˆë¬¸:
â€œê·¼ê±° ì—†ëŠ” ìì‹ ê°ì¸ê°€, ì•„ë‹ˆë©´ ëˆì´ ë˜ëŠ” ë¹„ë°€(Earned Secret)ì„ ì•Œê³  ìˆëŠ”ê°€?â€

í•„ìˆ˜ ì¦ê±°(ì—†ìœ¼ë©´ 3ì  ì´í•˜):
- Earned Secret (í˜„ì¥ì—ì„œë§Œ ì–»ì€ ë¬¸ì œ ì¸ì‚¬ì´íŠ¸)
- Founder-Market Fit
- ì†Œìˆ˜ë¼ë„ ì—´ê´‘í•˜ëŠ” ì´ˆê¸° ì‚¬ìš©ì ì‹ í˜¸

---

#### Series A
í•µì‹¬ ì§ˆë¬¸:
â€œë§ˆì¼€íŒ…ë¹„ë¡œ ë§Œë“  ê°€ì§œ ì„±ì¥ì´ ì•„ë‹Œê°€?â€

í•„ìˆ˜ ì¦ê±°(ì—†ìœ¼ë©´ 3ì  ì´í•˜):
- LTV/CAC â‰¥ 3
- ì½”í˜¸íŠ¸ ê¸°ë°˜ ë¦¬í…ì…˜
- GTM íš¨ìœ¨ì˜ ì‹œê³„ì—´ ê°œì„ 

---

#### Series B+
í•µì‹¬ ì§ˆë¬¸:
â€œê·œëª¨ê°€ ì»¤ì§ˆìˆ˜ë¡ ì´ìµë„ ì»¤ì§€ëŠ”ê°€?â€

í•„ìˆ˜ ì¦ê±°(ì—†ìœ¼ë©´ 3ì  ì´í•˜):
- NRR â‰¥ 110%
- ìš´ì˜ ë ˆë²„ë¦¬ì§€ ì¡´ì¬
- êµ¬ì¡°ì  ëª¨íŠ¸

---

### [B] ì‚°ì—…ë³„ ë³´ìˆ˜ì  ì£ëŒ€

#### SaaS / ê¸°ìˆ  / í”Œë«í¼
- Churn < 3%
- CAC Payback < 8~12ê°œì›”
- ìì²´ ë°ì´í„°/ì—”ì§„ ì—¬ë¶€

#### ì»¤ë¨¸ìŠ¤ / ë§ˆì¼“í”Œë ˆì´ìŠ¤
- CM2 í‘ì ì—¬ë¶€
- 3ê°œì›” ì¬êµ¬ë§¤ìœ¨ ì—…ê³„ í‰ê·  ëŒ€ë¹„ 1.5ë°°

#### ë°”ì´ì˜¤ / í—¬ìŠ¤ì¼€ì–´ / ë”¥í…Œí¬
- ê·œì œ/ê¸‰ì—¬ ë¡œë“œë§µ ëª…í™•ì„±
- ë¹„êµ ì„ìƒ/ì‹¤í—˜ ë°ì´í„°

---

### [C] ë¹„ì¦ˆë‹ˆìŠ¤ëª¨ë¸ë³„ í•µì‹¬ íŒë‹¨
- êµ¬ë…í˜•: ë¦¬í…ì…˜, NRR, ë‹¨ìœ„ê²½ì œì„±
- ê±°ë˜í˜•: GMV Ã— ë¹ˆë„ Ã— ë§ˆì§„
- ê´‘ê³ í˜•: ì°¸ì—¬ë„, ARPU, ë„¤íŠ¸ì›Œí¬ íš¨ê³¼
- ë¼ì´ì„ ìŠ¤: ê³„ì•½ êµ¬ì¡°, ë§ˆì¼ìŠ¤í†¤
- í•˜ë“œì›¨ì–´: ì›ê°€, ë§ˆì§„, ìŠ¤ì¼€ì¼ êµ¬ì¡°
"""


def get_api_key() -> str:
    api_key = st.secrets.get("gemini", {}).get("api_key")
    if not api_key:
        raise RuntimeError("Missing gemini api key in Streamlit secrets")
    return api_key


def kst_now() -> str:
    kst = tz.gettz("Asia/Seoul")
    return datetime.now(tz=kst).strftime("%Y-%m-%d %H:%M:%S")


def cache_key_for(content: str, step1_hash: str, step2_hash: str) -> str:
    parts = [md5_text(content), step1_hash, step2_hash, MODEL_NAME]
    return hashlib.sha256("::".join(parts).encode("utf-8")).hexdigest()


def compute_perspective_scores(step1: Dict[str, Any], step2: Optional[Dict[str, Any]]) -> Dict[str, int]:
    logic_score = float(step1.get("logic_score", 0) or 0)
    if step2:
        stage = float(step2.get("stage_score", 0) or 0)
        industry = float(step2.get("industry_score", 0) or 0)
        bm = float(step2.get("bm_score", 0) or 0)
        normalized_step2 = (stage + industry + bm) / 30.0 * 100.0
    else:
        normalized_step2 = 0.0
    critical = 0.8 * logic_score + 0.2 * normalized_step2
    neutral = 0.7 * logic_score + 0.3 * normalized_step2
    positive = 0.6 * logic_score + 0.4 * normalized_step2
    return {
        "critical": min(92, int(round(critical))),
        "neutral": min(92, int(round(neutral))),
        "positive": min(92, int(round(positive))),
    }


def recommendation_for(score: int) -> str:
    if score >= 80:
        return "ì¶”ì²œ"
    if score >= 70:
        return "ì¡°ê±´ë¶€ ê¶Œì¥"
    return "ë³´ë¥˜"


def derive_recommendations(scores: Dict[str, int]) -> Dict[str, str]:
    return {k: recommendation_for(v) for k, v in scores.items()}


def format_error_info(exc: Exception, file_name: str) -> Dict[str, str]:
    message = str(exc).replace("\n", " ")[:300]
    return {
        "type": exc.__class__.__name__,
        "message": message,
        "file_name": file_name,
    }


def evaluate_one(
    evaluator: Evaluator,
    content: str,
    file_name: str,
    step1_hash: str,
    step2_hash: str,
    force_rerun: bool,
    cache: Dict[str, Any],
) -> Dict[str, Any]:
    key = cache_key_for(content, step1_hash, step2_hash)
    if key in cache and not force_rerun:
        return {"status": STATUS_SKIPPED, "cache": cache[key], "file_name": file_name}

    step1_json = evaluator.evaluate_step1(
        content=content,
        prompt_step1=f"{BASE_PROMPT}\n\n{PROMPT_APPENDIX}",
        schema_hint_step1=to_json(STEP1_SCHEMA_HINT),
    )
    logic_score = float(step1_json.get("logic_score", 0) or 0)
    step1_json["pass_gate"] = logic_score >= 80

    step2_json: Optional[Dict[str, Any]] = None
    if step1_json.get("pass_gate", False):
        step2_json = evaluator.evaluate_step2(
            content=content,
            prompt_step2=f"{BASE_PROMPT}\n\n{PROMPT_APPENDIX}",
            schema_hint_step2=to_json(STEP2_SCHEMA_HINT),
            step1_json=step1_json,
        )

    scores = compute_perspective_scores(step1_json, step2_json)
    recommendations = derive_recommendations(scores)
    final_verdict = recommendations.get("critical", "ë³´ë¥˜")
    report_md = render_report(
        file_name,
        step1_json,
        step2_json,
        scores,
        recommendations,
        final_verdict,
    )

    result_payload = {
        "file_name": file_name,
        "timestamp": kst_now(),
        "company_name": step1_json.get("company_name", ""),
        "company_description": step1_json.get("one_line_summary", ""),
        "scores": scores,
        "recommendations": recommendations,
        "final_verdict": final_verdict,
        "overall_summary": step1_json.get("overall_summary", ""),
        "item_evaluations": step1_json.get("item_evaluations", {}),
        "strengths": step1_json.get("strengths", {}),
        "weaknesses": step1_json.get("weaknesses", {}),
        "red_flags": step1_json.get("red_flags", []),
        "axis_scores": {
            "stage": step2_json.get("stage_score") if step2_json else "",
            "industry": step2_json.get("industry_score") if step2_json else "",
            "bm": step2_json.get("bm_score") if step2_json else "",
        },
        "axis_comments": step2_json.get("axis_comments") if step2_json else {},
        "validation_questions": step2_json.get("validation_questions") if step2_json else {},
        "step1_json": step1_json,
        "step2_json": step2_json,
    }

    cache_entry = {
        "file_name": file_name,
        "timestamp": kst_now(),
        "step1": step1_json,
        "step2": step2_json,
        "report_md": report_md,
        "result_json": result_payload,
        "perspective_scores": scores,
        "recommendations": recommendations,
        "final_verdict": final_verdict,
        "status": STATUS_DONE,
        "cache_key": key,
    }
    cache[key] = cache_entry
    return {"status": STATUS_DONE, "cache": cache_entry, "file_name": file_name}


def build_sheet_row(entry: Dict[str, Any]) -> Dict[str, Any]:
    step1 = entry.get("step1", {})
    scores = entry.get("perspective_scores", {})
    recommendations = entry.get("recommendations", {})
    step2 = entry.get("step2", {})
    return {
        "timestamp(KST)": entry.get("timestamp", kst_now()),
        "file_name": entry.get("file_name", ""),
        "company_name": step1.get("company_name", ""),
        "company_description": step1.get("one_line_summary", ""),
        "score_critical": scores.get("critical", ""),
        "score_neutral": scores.get("neutral", ""),
        "score_positive": scores.get("positive", ""),
        "recommendation_critical": recommendations.get("critical", ""),
        "recommendation_neutral": recommendations.get("neutral", ""),
        "recommendation_positive": recommendations.get("positive", ""),
        "overall_summary": step1.get("overall_summary", ""),
        "item_evaluations_json": json.dumps(step1.get("item_evaluations", {}), ensure_ascii=True),
        "strengths_json": json.dumps(step1.get("strengths", {}), ensure_ascii=True),
        "weaknesses_json": json.dumps(step1.get("weaknesses", {}), ensure_ascii=True),
        "red_flags_json": json.dumps(step1.get("red_flags", []), ensure_ascii=True),
        "axis_scores_json": json.dumps(
            {
                "stage": step2.get("stage_score", "") if isinstance(step2, dict) else "",
                "industry": step2.get("industry_score", "") if isinstance(step2, dict) else "",
                "bm": step2.get("bm_score", "") if isinstance(step2, dict) else "",
            },
            ensure_ascii=True,
        ),
        "axis_comments_json": json.dumps(step2.get("axis_comments", {}) if isinstance(step2, dict) else {}, ensure_ascii=True),
        "validation_questions_json": json.dumps(
            step2.get("validation_questions", {}) if isinstance(step2, dict) else {}, ensure_ascii=True
        ),
        "final_verdict": entry.get("final_verdict", ""),
    }


def cache_to_excel_bytes(cache: Dict[str, Any]) -> bytes:
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "IR_EVAL"
    ws.append(SHEET_COLUMNS)
    for entry in cache.values():
        row = build_sheet_row(entry)
        ws.append([row.get(col, "") for col in SHEET_COLUMNS])
    buffer = BytesIO()
    wb.save(buffer)
    return buffer.getvalue()


def excel_filename() -> str:
    stamp = datetime.utcnow().strftime("%Y%m%d_%H%M")
    return f"ir_eval_{stamp}.xlsx"


def status_badge(status: str) -> str:
    mapping = {
        STATUS_DONE: "âœ…ì™„ë£Œ",
        STATUS_PENDING: "ğŸ•’ëŒ€ê¸°",
        STATUS_FAILED: "âš ï¸ì‹¤íŒ¨",
        STATUS_RUNNING: "ğŸ”„ì§„í–‰ì¤‘",
        STATUS_SKIPPED: "âœ…ì™„ë£Œ",
    }
    return mapping.get(status, status or "-")


def init_session_state() -> None:
    st.session_state.setdefault("files", [])
    st.session_state.setdefault("cache", {})
    st.session_state.setdefault("status_map", {})
    st.session_state.setdefault("selected_file_ids", [])
    st.session_state.setdefault("selected_file_name", "")
    st.session_state.setdefault("page", 1)


def main() -> None:
    st.set_page_config(page_title="IR Evaluator", layout="wide")
    st.title("Title : IR ë¶„ì„ & í‰ê°€")

    try:
        api_key = get_api_key()
    except RuntimeError as exc:
        st.error(str(exc))
        st.stop()

    init_session_state()

    top_cols = st.columns([4, 1, 1, 1, 1], gap="small")
    uploaded_files = top_cols[0].file_uploader(
        "Google drive í´ë” ID",
        type=["md"],
        accept_multiple_files=True,
    )
    scan_clicked = top_cols[1].button("ë¬¸ì„œ ìŠ¤ìº”")
    force_rerun = top_cols[2].checkbox("ìºì‹œ ë¬´ì‹œ(ì¬í‰ê°€)", value=False)
    refresh_clicked = top_cols[3].button("ìºì‹œ ìƒˆë¡œê³ ì¹¨")
    delete_cache_clicked = top_cols[4].button("ìºì‹œ ì‚­ì œ")

    if refresh_clicked:
        st.session_state["status_map"] = st.session_state.get("status_map", {})

    if delete_cache_clicked:
        st.session_state["cache"] = {}
        st.session_state["status_map"] = {}

    if scan_clicked and uploaded_files:
        st.session_state["files"] = uploaded_files
        st.session_state["status_map"] = {f.name: STATUS_PENDING for f in uploaded_files}

    files = st.session_state.get("files", [])
    if not files:
        st.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ .md íŒŒì¼ ëª©ë¡ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.")
        return

    table_header = st.columns([3, 1], gap="small")
    table_header[0].subheader("íŒŒì¼ ëª©ë¡ & IR List")
    if st.session_state.get("cache"):
        excel_bytes = cache_to_excel_bytes(st.session_state["cache"])
        table_header[1].download_button(
            label="ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=excel_bytes,
            file_name=excel_filename(),
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    else:
        table_header[1].button("ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", disabled=True)

    search_term = st.text_input("ê²€ìƒ‰(íŒŒì¼ëª…/ê¸°ì—…ëª…)", value="")

    header_cols = st.columns([3, 1, 1, 1, 1, 1, 1, 1, 1], gap="small")
    header_cols[0].markdown("**íŒŒì¼ëª…**")
    header_cols[1].markdown("**ì§„í–‰**")
    header_cols[2].markdown("**ì„ íƒ**")
    header_cols[3].markdown("**ê¸°ì—…ëª…**")
    header_cols[4].markdown("**critical**")
    header_cols[5].markdown("**neutral**")
    header_cols[6].markdown("**positive**")
    header_cols[7].markdown("**ë¯¸ë¦¬ë³´ê¸°**")
    header_cols[8].markdown("**íŒŒì¼ì—´ê¸°**")

    selected_ids = set(st.session_state.get("selected_file_ids", []))
    cache = st.session_state.get("cache", {})
    cache_by_name = {entry.get("file_name", ""): entry for entry in cache.values()}

    filtered_files = []
    for f in files:
        entry = cache_by_name.get(f.name)
        company_name = entry.get("step1", {}).get("company_name", "") if entry else ""
        if search_term:
            term = search_term.strip().lower()
            if term not in f.name.lower() and term not in company_name.lower():
                continue
        filtered_files.append(f)

    page_size = 10
    total_pages = max(1, (len(filtered_files) + page_size - 1) // page_size)
    page = min(st.session_state.get("page", 1), total_pages)
    pager_cols = st.columns([1, 1, 2, 1, 1], gap="small")
    if pager_cols[0].button("ì´ì „"):
        page = max(1, page - 1)
    pager_cols[2].markdown(f"í˜ì´ì§€ {page}/{total_pages}")
    if pager_cols[4].button("ë‹¤ìŒ"):
        page = min(total_pages, page + 1)
    st.session_state["page"] = page

    start = (page - 1) * page_size
    end = start + page_size
    for f in filtered_files[start:end]:
        entry = cache_by_name.get(f.name)
        company_name = entry.get("step1", {}).get("company_name", "") if entry else ""
        scores = entry.get("perspective_scores", {}) if entry else {}

        row = st.columns([3, 1, 1, 1, 1, 1, 1, 1, 1], gap="small")
        row[0].write(f.name)
        row[1].write(status_badge(st.session_state["status_map"].get(f.name, STATUS_PENDING)))
        checked = row[2].checkbox(
            "",
            value=f.name in selected_ids,
            key=f"select_{f.name}",
        )
        if checked:
            selected_ids.add(f.name)
        else:
            selected_ids.discard(f.name)
        row[3].write(company_name)
        row[4].write(scores.get("critical", ""))
        row[5].write(scores.get("neutral", ""))
        row[6].write(scores.get("positive", ""))
        if row[7].button("ë³´ê¸°", key=f"preview_{f.name}") and entry:
            st.session_state["selected_file_name"] = f.name
        report_text = entry.get("report_md", "") if entry else ""
        row[8].download_button(
            label="íŒŒì¼ì—´ê¸°",
            data=report_text or "",
            file_name=f"{f.name}.report.md",
            mime="text/markdown",
            key=f"dl_{f.name}",
        )

    st.session_state["selected_file_ids"] = list(selected_ids)

    action_cols = st.columns([6, 1, 1, 1], gap="small")
    evaluate_selected = action_cols[1].button("ì„ íƒ í‰ê°€")
    evaluate_all = action_cols[2].button("ì „ì²´ í‰ê°€")
    load_history = action_cols[3].button("íˆìŠ¤í† ë¦¬")

    evaluator = Evaluator(api_key=api_key, semaphore=threading.Semaphore(2))
    prompt_step1 = BASE_PROMPT
    prompt_step2 = BASE_PROMPT
    step1_hash = hash_prompt(prompt_step1)
    step2_hash = hash_prompt(prompt_step2)

    if evaluate_selected or evaluate_all:
        target_files = filtered_files if evaluate_all else [f for f in files if f.name in selected_ids]
        if not target_files:
            st.warning("í‰ê°€í•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.")
            return

        results: List[Dict[str, Any]] = []
        progress = st.progress(0)
        progress_text = st.empty()
        completed = 0

        def run_file(file_obj):
            content = file_obj.getvalue().decode("utf-8", errors="replace")
            return evaluate_one(
                evaluator,
                content,
                file_obj.name,
                step1_hash,
                step2_hash,
                force_rerun,
                cache,
            )

        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(run_file, f) for f in target_files]
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as exc:
                    results.append({"status": STATUS_FAILED, "error": format_error_info(exc, "")})
                completed += 1
                progress.progress(completed / len(target_files))
                progress_text.write(f"ì§„í–‰: {completed}/{len(target_files)}")

        for res in results:
            file_name = res.get("file_name", "")
            if res.get("status") == STATUS_DONE:
                st.session_state["status_map"][file_name] = STATUS_DONE
            elif res.get("status") == STATUS_SKIPPED:
                st.session_state["status_map"][file_name] = STATUS_SKIPPED
            else:
                st.session_state["status_map"][file_name] = STATUS_FAILED

    if load_history:
        st.info("ì„¸ì…˜ ìºì‹œ ê¸°ì¤€ìœ¼ë¡œ íˆìŠ¤í† ë¦¬ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")

    selected_name = st.session_state.get("selected_file_name")
    entry = cache_by_name.get(selected_name) if selected_name else None
    render_preview_panel(entry)


if __name__ == "__main__":
    main()
